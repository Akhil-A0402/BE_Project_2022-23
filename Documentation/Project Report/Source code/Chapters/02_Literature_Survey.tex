\chapter{Literature Survey}

\\

The main aim of the speech-to-speech model attempts is to provide one language to the required translated languages and giving again the output in speech. This chapter gives better insights into the project through the analysis done on various research papers related to the Translation
There are various techniques as well as various models/ algorithms to perform the translation of languages
\newpage

\section{Literature Survey}
\subsection{Literature Survey on Attention Is All You Need}
{\normalsize{\par The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best-performing models also connect the encoder and decoder through an attention mechanism. 
\par They propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.
\par Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.
\newline
\\
{\textbf{Methodology Used:}} Recurrent neural networks, long short-term memory with an attention mechanism
} }
\newline
\\
{\textbf{Reference:} Attention Is All You Need -  Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com}

\newpage

\subsection{Literature Survey on Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}
{\normalsize {\par NMT systems are known to be computationally expensive both in training and in translation. NMT systems lack robustness, particularly when input sentences contain rare words. These issues have hindered NMT’s use in practical deployments and services, where both accuracy and speed are essential.
\par This paper consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To improve handling of rare words, we divide words into a limited set of common sub-word units (“word pieces”) for both input and output.
\par This method provides a good balance between the flexibility of “character”-delimited models and the efficiency of “word”-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. 
\\
\newline
{\textbf{Methodology used:}} Neural Machine Translation System}}
\newline
\\
{\textbf{Reference:} Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation - Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi yonghui,schuster,zhifengc,qvl,mnorouzi@google.com}
\subsection{Literature Survey on Applications Research of Machine Learning Algorithm in Translation System }
{\normalsize{\par Machine translation has significantly evolved over time, especially in terms of accuracy levels in its output.Machine translation is a study spanning many fields. Machine translation rules system can be divided into two categories, binary translation rules and ternary translation rules.
\par The design and implementation of the Chinese English neural machine translation system mainly includes pre-processing module, encoding and decoding module and attention module. The sequence of each module in accordance with the system includes pre-processing module, encoding Overall, divide and conquer the longest noun phrase and sentence frame by the divide and conquer strategy, and in part use the method of multi sequence coding to translate the longest noun phrase and sentence frame.
\par These are mostly used by individual consumers, such as travelers, students, etc. Real-time translation applications most commonly offer:
\newline
{\textbf{Refrences:}}
[1] Haque A U, Mandal P, Meng J, et al. Wind speed forecast model for wind farm based on a hybrid machine learning algorithm[J]. International Journal of Sustainable Energy, 2015, 34(1): 38- 51. 
[2] Bahar P, Alkhouli T, Peter J T, et al. Empirical investigation of optimization algorithms in neural machine translation[J]. The Prague Bulletin of Mathematical Linguistics, 2017, 108(1): 13- 25
}}
}}
\clearpage
\subsection{Literature Survey Unsupervised Neural Machine Translation With Cross-Lingual Language Representation Agreement}
{\normalsize{\par Unsupervised NMT is a challenging scenario, however it may allow to overcome some of the most pressing problems related to MT for low-resource languages. Although the field of UNMT is fairly new, it has been evolving rapidly.\par Current UNMT models achieve results that are promising, yet we think that the performance of UNMT can be further improved with a better initialization. Cross-lingual embeddings (CLEs) allow to learn multilingual word representations.\par CLEs are important components that facilitate cross-lingual transfer in current NLP models and proved to be very useful in downstream NLP tasks (such as MT). Importantly, CLEs help to bridge the gap between resource-rich and low-resource languages, because CLEs can be trained even without using any parallel corpora}}
\clearpage
\subsection{Literature Survey on Overview of Neural Machine Translation for English - Hindi}
{\normalsize{\par In this paper they used Word2Vec model that processes text. Word2Vec creates vectors that are distributed numerical illustrations of word features, features such as the context of singular words.\par This algorithm has two flavors CBOW and Skip-Gram. The purpose of Skip-gram is to predict source context-words from the target words. Then we use POS-Tagging in which if the word is present in sample database then we directly assign the POS-tag from the database. Then we can use exact matching or fuzzy matching. For the Translation reordered rule is extracted from the databases then that rule is applied on source sentence which translates words from sorce language to required languages.\par The evaluation of this method is, if the model is of short messages then you get upto 85 percent accurate result but for lengthy messages the result is of 60 percent accurate results.}} \\
{\textbf{Reference:}  Shashi Pal Singh, Ajai Kumar, “Machine Translation using Deep Learning: An Overview”, International Conference on Computer, Communications and Electronics, 2017.  Hugo Larochelle, Yoshua Bengio, Jérôme Louradour, Pascal Lamblin, “Exploring strategies for training deep neural networks”, Journal of Machine Learning Research 1 (2009) submitted 12/07.
}
\clearpage
\subsection{Literature Survey on Paper 6}
\subsection{Literature Survey on Paper 7}
\subsection{Literature Survey on Paper 8}

\section{Summary of Literature Survey}
\section{Gap Analysis}
\begin{itemize}
    \item The more dataset we create for this model, the more accurate the model will be.
    \item The subtask we are performing are very domain specific so a lot of study is done on the topic.
\end{itemize}